{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNLQtYHuPaUJpFVRrkv20Y1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadElf1337/jax-gpt2/blob/main/jax_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WGYMC5mBnIU"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Optional, Tuple, Callable\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.typing import DTypeLike as Dtype\n",
        "import flax.linen as nnx\n",
        "from flax.core import FrozenDict, freeze, unfreeze\n",
        "from flax.traverse_util import flatten_dict, unflatten_dict\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "j51d6l5Oola9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class GPTConfig:\n",
        "    ctx_len: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layers: int = 12\n",
        "    n_heads: int = 12\n",
        "    n_embeds: int = 768\n",
        "    dropout_rate: float = 0.1\n",
        "    use_bias: bool = True\n",
        "    dtype: Optional[str] = None"
      ],
      "metadata": {
        "id": "b7zC6s1vxK0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nnx.Module):\n",
        "\n",
        "    def __init__(self, config: GPTConfig, deterministic: Optional[bool] = None, use_proj_bias: bool = True):\n",
        "        self.config = config\n",
        "        self.use_proj_bias = use_proj_bias\n",
        "        self.dtype = config.dtype\n",
        "        self.deterministic = deterministic\n",
        "\n",
        "    def __call__(self, config: GPTConfig, x, mask, deterministic=None):\n",
        "        B, T, C = x.shape\n",
        "        assert C % config.n_heads == 0\n",
        "        head_dim = C // config.n_heads\n",
        "        deterministic = nnx.merge('deterministic', self.deterministic, deterministic)\n",
        "\n",
        "        qkv = nnx.Linear(C, 3*C, use_bias=self.use_proj_bias, dtype=config.dtype, name='c_attn')(x)\n",
        "        qkv = jnp.einsum('ijk->ij(hd)', qkv, h=3*config.n_heads, d=head_dim)\n",
        "        q, k, v = jnp.array_split(qkv, 3, axis=2)\n",
        "        scale = 1.0 / jnp.sqrt(head_dim).astype(config.dtype)\n",
        "        attn = jnp.einsum('bthd,bshd->bths', q, k)*scale\n",
        "        attn = jnp.where(mask, attn, jnp.finfo(self.dtype).min)\n",
        "        attn = nnx.softmax(attn).astype(self.dtype)\n",
        "        attn = nnx.Dropout(config.dropout_rate)(attn, deterministic=deterministic)\n",
        "\n",
        "        x = jnp.einsum('bths, bshd->bt(hd)', attn, v)\n",
        "        x = nnx.Linear(3*C, C, use_bias=self.use_proj_bias, dtype=config.dtype, name='c_proj')(x)\n",
        "        x = nnx.Dropout(rate=config.dropout_rate)(x, deterministic=deterministic)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fdq0Dy1HEW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nnx.Module):\n",
        "    def __init__(self, config, deterministic: Optional[bool] = None):\n",
        "        self.deterministic = deterministic\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, x, config: GPTConfig, deterministic=None):\n",
        "        B, T, C = x.shape\n",
        "        x = nnx.Linear(C, 4*C, dtype=config.dtype, use_bias=config.use_bias, name='c_fc')(x)\n",
        "        x = nnx.gelu(x, approximate=True)\n",
        "        x = nnx.Linear(4*C, C, dtype=config.dtype, use_bias=config.use_bias, name='c_proj')(x)\n",
        "        x = nnx.Dropout(config.dropout_rate)(x, deterministic)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LFVIxlapPAhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nnx.Module):\n",
        "    config: GPTConfig\n",
        "\n",
        "    def __init__(self, config: GPTConfig, deterministic: Optional[bool] = None):\n",
        "        self.config = config\n",
        "        self.ln_1 = nnx.LayerNorm(epsilon=1e-5, dtype=config.dtype, use_bias=config.use_bias)\n",
        "        self.attn = CausalSelfAttention(config.n_heads, config.dtype, dropout_rate=config.dropout_rate)\n",
        "        self.ln_2 = nnx.LayerNorm(epsilon=1e-5, dtype=config.dtype, use_bias=config.use_bias)\n",
        "        self.ffn = FFN(config)\n",
        "\n",
        "    def __call__(self, x, mask=None, deterministic=None):\n",
        "        x = x + self.attn(self.ln_1(x), mask, deterministic)\n",
        "        x = x + self.ffn(self.ln_2(x), deterministic)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "CcG7rEKR4ESh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_attention_mask(query_input: jax.Array,\n",
        "                        key_input: jax.Array,\n",
        "                        pairwise_fn: Callable[..., Any] = jnp.multiply,\n",
        "                        extra_batch_dims: int = 0,\n",
        "                        dtype: Dtype = jnp.float32):\n",
        "    \"\"\"Mask-making helper for attention weights.\"\"\"\n",
        "    mask = pairwise_fn(jnp.expand_dims(query_input, axis=-1),\n",
        "                       jnp.expand_dims(key_input, axis=-2))\n",
        "    mask = jnp.expand_dims(mask, axis=-3)\n",
        "    mask = jnp.expand_dims(mask, axis=tuple(range(extra_batch_dims)))\n",
        "    return mask.astype(dtype)\n",
        "\n",
        "def make_causal_mask(x: jax.Array, extra_batch_dims: int = 0, dtype: Dtype = jnp.float32) -> jax.Array:\n",
        "    \"\"\"Make a causal mask for self-attention.\"\"\"\n",
        "    idxs = jnp.broadcast_to(jnp.arange(x.shape[-1], dtype=jnp.int32), x.shape)\n",
        "    return make_attention_mask(idxs, idxs, jnp.greater_equal, extra_batch_dims=extra_batch_dims, dtype=dtype)"
      ],
      "metadata": {
        "id": "6JGBbpBV8W7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nnx.Module):  # Inherit from nnx.Module\n",
        "    config: GPTConfig\n",
        "\n",
        "    def setup(self):  # Use setup() for module initialization\n",
        "        # Initialize wte and wpe here, no need for conditional logic inside __call__\n",
        "        self.wte = nnx.Embed(self.config.vocab_size, self.config.n_embeds, dtype=self.config.dtype, name='wte')\n",
        "        self.wpe = nnx.Embed(self.config.ctx_len, self.config.n_embeds, dtype=self.config.dtype, name='wpe')\n",
        "        self.blocks = [\n",
        "            Block(self.config, name=str(i)) for i in range(self.config.n_layers)\n",
        "        ]\n",
        "        self.ln_f = nnx.LayerNorm(epsilon=1e-5, dtype=self.config.dtype, use_bias=self.config.use_bias, name='ln_f')\n",
        "\n",
        "\n",
        "    def __call__(self, idx, deterministic=None):\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.config.ctx_len\n",
        "\n",
        "        pos = jnp.arange(0, T)[None]\n",
        "        attn_mask = make_causal_mask(idx, dtype=bool)\n",
        "\n",
        "        token_embeddings = self.wte(idx)  # Use self.wte\n",
        "        pos_embeddings = self.wpe(pos)  # Use self.wpe and pos\n",
        "\n",
        "        x = nnx.Dropout(rate=self.config.dropout_rate)(token_embeddings + pos_embeddings, deterministic=deterministic)\n",
        "\n",
        "        for block in self.blocks:  # Iterate through blocks\n",
        "            x = block(x, attn_mask, deterministic=deterministic)\n",
        "\n",
        "        x = self.ln_f(x)  # Use self.ln_f\n",
        "\n",
        "        logits = jnp.einsum('...d,vd->...v', self.wte.variables['params']['embedding'], x)  # Access embedding directly\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "7li_geHH54Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYivlTTGUhKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}